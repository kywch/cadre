---
layout: post
title: "The ongoing consolidation in AI is incredible"
author: Andrej Karpathy
date: 2021-12-07
categories: ml/ai/nlp
tags: transformer
---

[https://twitter.com/karpathy/status/1468370605229547522](https://twitter.com/karpathy/status/1468370605229547522)

> When I started ~decade ago vision, speech, natural language, reinforcement learning, etc. were completely separate; You couldn't read papers across areas - the approaches were completely different, often not even ML based.
>
> But as of approx. last two years, even the neural net architectures across all areas are starting to look identical - a Transformer (definable in ~200 lines of PyTorch [https://github.com/karpathy/minGPT/blob/master/mingpt/model.pyâ€¦](https://t.co/xQL5NyJkLE)), with very minor differences. Either as a strong baseline or (often) state of the art.
>
> You can feed it sequences of words. Or sequences of image patches. Or sequences of speech pieces. Or sequences of (state, action, reward) in reinforcement learning. You can throw in arbitrary other tokens into the conditioning set - an extremely simple/flexible modeling framework
>
> So even though I'm technically in vision, papers, people and ideas across all of AI are suddenly extremely relevant. Everyone is working with essentially the same model, so most improvements and ideas can "copy paste" rapidly across all of AI.
