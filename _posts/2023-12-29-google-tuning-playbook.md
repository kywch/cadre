---
layout: post
title: "Deep learning tuning playbook"
author: Varun Godbole, George E. Dahl, Justin Gilmer, Christopher J. Shallue, Zachary Nado
date: 2023-01-19
categories: ml/ai/nlp
tags: large-language-model
---

[https://github.com/google-research/tuning_playbook](https://github.com/google-research/tuning_playbook)

> Deep Learning Tuning Playbook

> Currently, there is an astonishing amount of toil and guesswork involved in actually getting deep neural networks to work well in practice. Even worse, the actual recipes people use to get good results with deep learning are rarely documented. Papers gloss over the process that led to their final results in order to present a cleaner story, and machine learning engineers working on commercial problems rarely have time to take a step back and generalize their process. Textbooks tend to eschew practical guidance and prioritize fundamental principles, even if their authors have the necessary experience in applied work to provide useful advice. When preparing to create this document, we couldn't find any comprehensive attempt to actually explain how to get good results with deep learning. Instead, we found snippets of advice in blog posts and on social media, tricks peeking out of the appendix of research papers, occasional case studies about one particular project or pipeline, and a lot of confusion.

> There is a vast gulf between the results achieved by deep learning experts and less skilled practitioners using superficially
similar methods. At the same time, these very experts readily admit some of what they do might not be well-justified. As deep learning matures and has a larger impact on the world, the community needs more resources covering useful recipes, including all the practical details that can be so critical for obtaining good results.
